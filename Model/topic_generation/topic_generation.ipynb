{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['', '亲情', '痛苦', '爱情', '青春', '失恋', '友情', '兄弟', '背叛', '成功', '坚持', '告诫', '失败', '迷茫', '艰难不易', '过去', '回忆', '成长', '将来', '金钱', '物欲', '努力', 'diss', '讨厌', '憎恶', '虚伪', '积极', '生活', '中国风', '耍帅'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "dict_topic = {0.0: '',\n",
    "              1.0: '亲情', 1.1: '痛苦',\n",
    "              2.0: '爱情', 2.1: '青春', 2.2: '失恋',\n",
    "              3.0: '友情', 3.1: '兄弟', 3.2: '背叛',\n",
    "              4.0: '成功', 4.1: '坚持', 4.2: '告诫',\n",
    "              5.0: '失败', 5.1: '迷茫', 5.2: '艰难不易',\n",
    "              6.0: '过去', 6.1: '回忆', 6.2: '成长',\n",
    "              7.0: '将来',\n",
    "              8.0: '金钱', 8.1: '物欲',\n",
    "              9.0: '努力',\n",
    "              10.0: 'diss', 10.1: '讨厌', 10.2: '憎恶', 10.3: '虚伪',\n",
    "              11.0: '积极',\n",
    "              12.0: '生活',\n",
    "              13.0: '中国风',\n",
    "              14.0: '耍帅'}\n",
    "# data_topic = pd.read_csv('data _topic.csv')\n",
    "dict_topic.values()\n",
    "# prime = \"dddd\"\n",
    "# if prime not in dict_topic.values():\n",
    "#     raise ValueError('Topic Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>腿 搁在办公桌上</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>这五六分钟我只关心说唱</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>大脑像硬币投进的湖里的波浪激荡</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>如果你们智商170和我一样</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>耳边的铃铛 晶晶亮 轻轻唱 interesting的声音记在了心上</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>倒掉这鸡汤 错的路走几趟 开心难过换来换去像是在打乒乓</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>叮当猫口袋里频临乓啷在响 不像一无所有的人只好拿出了抢</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bang bang 什么东西不能用道理讲 man down 谁站出来说他还有理想</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>这景象 让我联想 我的思路像是蜻蜓 在飞行 听不到你演讲</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>我拼命地前行 不断遇到瓶颈 全程保持清醒 时刻准备面对劲敌</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>把事情办到 懒得跟你探讨 省着我的弹药 不叽叽喳喳</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>嘻嘻哈哈 爸爸妈妈不喜欢嘻哈 不喜欢我说脏话</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>但脏话的背后是真实的揭露 跟上这节奏和生活邂逅</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>关于这个时空 维度不只时钟 做了一个梦里碰到一个神对那个秘密精通</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>群居动物 韵律和音乐 所以人都会孤独 革命和鲜血</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>千真万确的不千真万确 欢迎来到这个不确定的世界</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>不相信这一切 是全部的一切 渴望真相的心特别炽烈</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>尽管早晚 都会凋谢 尽管没有希望还是要去了解</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>像zombie 下象棋 趁你情敌 把你将军</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>所有的输赢 在刹那的决定</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>所有的决定 都是注定</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>所有的fans跟着伴奏 穿着yoga pants来战斗</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>女孩都爱Chuck Zigga Jiang他的汗臭 XZT渐快渐慢的flow</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>让你颤抖</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hype it up hates怕 倒吸口冷气 把我们比作了2 pac and biggie</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Feezy这场休息 看场好戏 SFG在夏季的ending迎来新的胜利</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>iPhone7 能防水不怕他们pussy get wetter</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>我比你厉害但我不consider you any lesser</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>海尔兄弟上了pigeons&amp;planes 全世界都听到中国嘻哈</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>奥运会上傅园慧告诉全世界她那天来了大姨夫</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5709</th>\n",
       "      <td>Uh 315到了</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5710</th>\n",
       "      <td>我会给你打电话</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5711</th>\n",
       "      <td>朋友多去读点音乐史</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5712</th>\n",
       "      <td>提高点情商 Huh</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5713</th>\n",
       "      <td>朋友我不混说唱圈</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5714</th>\n",
       "      <td>说唱居然还有圈</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>说唱居然还有圈</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716</th>\n",
       "      <td>曾经有一份真诚的爱情</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5717</th>\n",
       "      <td>摆在我的面前</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5718</th>\n",
       "      <td>但是我没有珍惜</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5719</th>\n",
       "      <td>沉默只是不想反驳</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5720</th>\n",
       "      <td>给个台阶自己下</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5721</th>\n",
       "      <td>根本就不是一个圈</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5722</th>\n",
       "      <td>没人想着会理解</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5723</th>\n",
       "      <td>看的不是一个天</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5724</th>\n",
       "      <td>你现实我想着实现</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5725</th>\n",
       "      <td>在属于我的世界</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>音乐多么直接</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5727</th>\n",
       "      <td>梦想虽然不着边际</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5728</th>\n",
       "      <td>你的浮华时间</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5729</th>\n",
       "      <td>嘴巴上上拉链</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5730</th>\n",
       "      <td>不想听你吹破天际</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5731</th>\n",
       "      <td>总有人对你行为喜好指指点点</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5732</th>\n",
       "      <td>可他却活在别人的嘴里</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>重复日日年年</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>从未规划人生时间挥发</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>而且活得小心</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>我却最大化习惯和生活较劲</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>做好了自己说的要实际</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5738</th>\n",
       "      <td>人生的日记记好每一笔</td>\n",
       "      <td>diss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5739 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lyric topic\n",
       "0                                           腿 搁在办公桌上  diss\n",
       "1                                        这五六分钟我只关心说唱  diss\n",
       "2                                    大脑像硬币投进的湖里的波浪激荡  diss\n",
       "3                                      如果你们智商170和我一样  diss\n",
       "4                  耳边的铃铛 晶晶亮 轻轻唱 interesting的声音记在了心上  diss\n",
       "5                        倒掉这鸡汤 错的路走几趟 开心难过换来换去像是在打乒乓  diss\n",
       "6                        叮当猫口袋里频临乓啷在响 不像一无所有的人只好拿出了抢  diss\n",
       "7           bang bang 什么东西不能用道理讲 man down 谁站出来说他还有理想  diss\n",
       "8                       这景象 让我联想 我的思路像是蜻蜓 在飞行 听不到你演讲  diss\n",
       "9                      我拼命地前行 不断遇到瓶颈 全程保持清醒 时刻准备面对劲敌  diss\n",
       "10                         把事情办到 懒得跟你探讨 省着我的弹药 不叽叽喳喳  diss\n",
       "11                            嘻嘻哈哈 爸爸妈妈不喜欢嘻哈 不喜欢我说脏话  diss\n",
       "12                           但脏话的背后是真实的揭露 跟上这节奏和生活邂逅  diss\n",
       "13                  关于这个时空 维度不只时钟 做了一个梦里碰到一个神对那个秘密精通  diss\n",
       "14                          群居动物 韵律和音乐 所以人都会孤独 革命和鲜血  diss\n",
       "15                           千真万确的不千真万确 欢迎来到这个不确定的世界  diss\n",
       "16                          不相信这一切 是全部的一切 渴望真相的心特别炽烈  diss\n",
       "17                            尽管早晚 都会凋谢 尽管没有希望还是要去了解  diss\n",
       "18                             像zombie 下象棋 趁你情敌 把你将军  diss\n",
       "19                                      所有的输赢 在刹那的决定  diss\n",
       "20                                        所有的决定 都是注定  diss\n",
       "21                       所有的fans跟着伴奏 穿着yoga pants来战斗  diss\n",
       "22            女孩都爱Chuck Zigga Jiang他的汗臭 XZT渐快渐慢的flow  diss\n",
       "23                                              让你颤抖  diss\n",
       "24    Hype it up hates怕 倒吸口冷气 把我们比作了2 pac and biggie  diss\n",
       "25                Feezy这场休息 看场好戏 SFG在夏季的ending迎来新的胜利  diss\n",
       "26                   iPhone7 能防水不怕他们pussy get wetter  diss\n",
       "27                   我比你厉害但我不consider you any lesser  diss\n",
       "28                   海尔兄弟上了pigeons&planes 全世界都听到中国嘻哈  diss\n",
       "29                              奥运会上傅园慧告诉全世界她那天来了大姨夫  diss\n",
       "...                                              ...   ...\n",
       "5709                                        Uh 315到了  diss\n",
       "5710                                         我会给你打电话  diss\n",
       "5711                                       朋友多去读点音乐史  diss\n",
       "5712                                       提高点情商 Huh  diss\n",
       "5713                                        朋友我不混说唱圈  diss\n",
       "5714                                         说唱居然还有圈  diss\n",
       "5715                                         说唱居然还有圈  diss\n",
       "5716                                      曾经有一份真诚的爱情  diss\n",
       "5717                                          摆在我的面前  diss\n",
       "5718                                         但是我没有珍惜  diss\n",
       "5719                                        沉默只是不想反驳  diss\n",
       "5720                                         给个台阶自己下  diss\n",
       "5721                                        根本就不是一个圈  diss\n",
       "5722                                         没人想着会理解  diss\n",
       "5723                                         看的不是一个天  diss\n",
       "5724                                        你现实我想着实现  diss\n",
       "5725                                         在属于我的世界  diss\n",
       "5726                                          音乐多么直接  diss\n",
       "5727                                        梦想虽然不着边际  diss\n",
       "5728                                          你的浮华时间  diss\n",
       "5729                                          嘴巴上上拉链  diss\n",
       "5730                                        不想听你吹破天际  diss\n",
       "5731                                   总有人对你行为喜好指指点点  diss\n",
       "5732                                      可他却活在别人的嘴里  diss\n",
       "5733                                          重复日日年年  diss\n",
       "5734                                      从未规划人生时间挥发  diss\n",
       "5735                                          而且活得小心  diss\n",
       "5736                                    我却最大化习惯和生活较劲  diss\n",
       "5737                                      做好了自己说的要实际  diss\n",
       "5738                                      人生的日记记好每一笔  diss\n",
       "\n",
       "[5739 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sys.stdout.write('>')\n",
    "# sys.stdout.flush()\n",
    "# prime = sys.stdin.readline()\n",
    "# prime = prime[:-1]\n",
    "prime = 'diss'\n",
    "\n",
    "data = data_topic[data_topic['topic'] == prime]\n",
    "data = data.reset_index(drop = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4003"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_index = random.randrange(0,len(data),1)\n",
    "rand_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我爱上陌生人'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[rand_index]['lyric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7dd7f1069289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'useful'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "profiler = Word2Vec()\n",
    "profiler.build_vocab(word_source)\n",
    "word_source = [\n",
    "    ['I', 'love', 'natural', 'language', 'processing'],\n",
    "    ['word2vec', 'is', 'a', 'useful', 'model']\n",
    "]\n",
    "profiler.train(word_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-17 15:18:42,734 : INFO : collecting all words and their counts\n",
      "2018-08-17 15:18:42,740 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-17 15:18:43,379 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
      "2018-08-17 15:18:43,963 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
      "2018-08-17 15:18:44,560 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
      "2018-08-17 15:18:45,091 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
      "2018-08-17 15:18:45,552 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
      "2018-08-17 15:18:45,883 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
      "2018-08-17 15:18:45,884 : INFO : Loading a fresh vocabulary\n",
      "2018-08-17 15:18:46,156 : INFO : min_count=5 retains 15173 unique words (27% of original 56057, drops 40884)\n",
      "2018-08-17 15:18:46,156 : INFO : min_count=5 leaves 1095086 word corpus (94% of original 1161192, drops 66106)\n",
      "2018-08-17 15:18:46,195 : INFO : deleting the raw counts dictionary of 56057 items\n",
      "2018-08-17 15:18:46,198 : INFO : sample=0.001 downsamples 42 most-common words\n",
      "2018-08-17 15:18:46,198 : INFO : downsampling leaves estimated 781596 word corpus (71.4% of prior 1095086)\n",
      "2018-08-17 15:18:46,236 : INFO : estimated required memory for 15173 words and 100 dimensions: 19724900 bytes\n",
      "2018-08-17 15:18:46,237 : INFO : resetting layer weights\n",
      "2018-08-17 15:18:46,427 : INFO : training model with 3 workers on 15173 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-08-17 15:18:47,456 : INFO : EPOCH 1 - PROGRESS: at 23.64% examples, 188778 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:48,461 : INFO : EPOCH 1 - PROGRESS: at 53.06% examples, 224150 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:49,507 : INFO : EPOCH 1 - PROGRESS: at 82.18% examples, 218364 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:50,146 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-17 15:18:50,147 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-17 15:18:50,149 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-17 15:18:50,150 : INFO : EPOCH - 1 : training on 1161192 raw words (781703 effective words) took 3.7s, 210218 effective words/s\n",
      "2018-08-17 15:18:51,169 : INFO : EPOCH 2 - PROGRESS: at 25.39% examples, 203974 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:52,200 : INFO : EPOCH 2 - PROGRESS: at 54.57% examples, 228797 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:53,210 : INFO : EPOCH 2 - PROGRESS: at 87.02% examples, 228598 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:53,559 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-17 15:18:53,561 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-17 15:18:53,562 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-17 15:18:53,563 : INFO : EPOCH - 2 : training on 1161192 raw words (782023 effective words) took 3.4s, 229328 effective words/s\n",
      "2018-08-17 15:18:54,580 : INFO : EPOCH 3 - PROGRESS: at 29.46% examples, 237492 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:55,582 : INFO : EPOCH 3 - PROGRESS: at 59.73% examples, 255659 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:56,588 : INFO : EPOCH 3 - PROGRESS: at 96.96% examples, 251358 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:56,672 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-17 15:18:56,675 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-17 15:18:56,676 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-17 15:18:56,676 : INFO : EPOCH - 3 : training on 1161192 raw words (781609 effective words) took 3.1s, 251196 effective words/s\n",
      "2018-08-17 15:18:57,695 : INFO : EPOCH 4 - PROGRESS: at 29.46% examples, 237256 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:58,712 : INFO : EPOCH 4 - PROGRESS: at 60.42% examples, 257095 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:59,731 : INFO : EPOCH 4 - PROGRESS: at 95.75% examples, 246685 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:18:59,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-17 15:18:59,870 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-17 15:18:59,872 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-17 15:18:59,872 : INFO : EPOCH - 4 : training on 1161192 raw words (781531 effective words) took 3.2s, 244735 effective words/s\n",
      "2018-08-17 15:19:00,893 : INFO : EPOCH 5 - PROGRESS: at 28.72% examples, 231411 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:19:01,895 : INFO : EPOCH 5 - PROGRESS: at 60.42% examples, 259311 words/s, in_qsize 0, out_qsize 0\n",
      "2018-08-17 15:19:02,914 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-17 15:19:02,917 : INFO : EPOCH 5 - PROGRESS: at 99.80% examples, 256822 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-17 15:19:02,918 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-17 15:19:02,919 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-17 15:19:02,919 : INFO : EPOCH - 5 : training on 1161192 raw words (782022 effective words) took 3.0s, 257216 effective words/s\n",
      "2018-08-17 15:19:02,920 : INFO : training on a 5805960 raw words (3908888 effective words) took 16.5s, 237019 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from nltk.corpus import brown\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "brown_vecs = word2vec.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1a2c915668>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.873669539555881"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_vecs.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '男人' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f9f57a832e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbrown_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'男人'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'女人'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 )\n\u001b[0;32m-> 1398\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \"\"\"\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.n_similarity() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \"\"\"\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '男人' not in vocabulary\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
